#!/usr/bin/env python3
"""
Simplified ML Model Training - Fixed Numpy Compatibility
Implements RandomForest, GradientBoosting, and LinearRegression with comprehensive evaluation
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# Set style for better plots
plt.style.use('default')
sns.set_palette("husl")

def load_data():
    """Load the enhanced dataset with all engineered features"""
    print("üìä Loading enhanced yacht market dataset...")
    df = pd.read_csv('enhanced_yacht_market_data.csv')
    
    # Select features that are safe for ML (no future leakage)
    feature_cols = [col for col in df.columns if col not in [
        'sale_price', 'asking_price_eur', 'sale_to_ask_ratio',
        'asking_price_usd', 'asking_price_gbp', 'sale_price_usd', 
        'sale_price_gbp'
    ]]
    
    # Remove identification and high-cardinality columns
    feature_cols = [col for col in feature_cols if col not in [
        'yacht_id', 'broker', 'listing_number', 'last_seen_date'
    ]]
    
    X = df[feature_cols].copy()
    y = df['sale_price'].copy()
    
    print(f"Original features: {len(feature_cols)}")
    print(f"Target variable: {y.name}")
    print(f"Dataset shape: {df.shape}")
    
    # Handle categorical variables
    categorical_cols = X.select_dtypes(include=['object', 'category']).columns
    print(f"Categorical columns: {list(categorical_cols)}")
    
    if len(categorical_cols) > 0:
        X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)
        print(f"After one-hot encoding: {X.shape[1]} features")
    
    # Fill missing values
    X.fillna(X.median(), inplace=True)
    
    return X, y, X.columns.tolist()

def train_models(X, y):
    """Train multiple ML models with cross-validation"""
    print("\nü§ñ Training ML models...")
    
    models = {
        'LinearRegression': LinearRegression(),
        'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),
        'GradientBoosting': GradientBoostingRegressor(n_estimators=100, random_state=42)
    }
    
    results = {}
    
    for name, model in models.items():
        print(f"Training {name}...")
        
        # Cross-validation
        cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')
        
        # Train-test split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        
        # Metrics
        results[name] = {
            'model': model,
            'cv_mean_r2': np.mean(cv_scores),
            'cv_std_r2': np.std(cv_scores),
            'test_r2': r2_score(y_test, y_pred),
            'test_mse': mean_squared_error(y_test, y_pred),
            'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred)),
            'test_mae': mean_absolute_error(y_test, y_pred),
            'predictions': y_pred,
            'actual': y_test.values
        }
        
        print(f"‚úÖ {name} Results:")
        print(f"  Cross-Validated R¬≤: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")
        print(f"  Test R¬≤: {results[name]['test_r2']:.4f}")
        print(f"  Test RMSE: ‚Ç¨{results[name]['test_rmse']:,.0f}")
        print(f"  Test MAE: ‚Ç¨{results[name]['test_mae']:,.0f}")
    
    return results

def visualize_model_comparison(results):
    """Create comprehensive model comparison visualizations"""
    print("\nüìà Creating visualizations...")
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # Model Performance Comparison
    model_names = list(results.keys())
    r2_scores = [results[name]['test_r2'] for name in model_names]
    rmse_scores = [results[name]['test_rmse'] for name in model_names]
    mae_scores = [results[name]['test_mae'] for name in model_names]
    
    x = np.arange(len(model_names))
    width = 0.25
    
    # Normalize RMSE and MAE for comparison
    max_score = max(max(r2_scores), max(rmse_scores/np.max(rmse_scores)), max(mae_scores/np.max(rmse_scores)))
    
    axes[0,0].bar(x - width, r2_scores/max_score, width, label='R¬≤ (normalized)', alpha=0.8)
    axes[0,0].bar(x, rmse_scores/np.max(rmse_scores), width, label='RMSE (scaled)', alpha=0.8)
    axes[0,0].bar(x + width, mae_scores/np.max(rmse_scores), width, label='MAE (scaled)', alpha=0.8)
    axes[0,0].set_xlabel('Models')
    axes[0,0].set_ylabel('Normalized Performance')
    axes[0,0].set_title('Model Performance Comparison')
    axes[0,0].set_xticks(x)
    axes[0,0].set_xticklabels(model_names)
    axes[0,0].legend()
    axes[0,0].grid(True, alpha=0.3)
    
    # Cross-Validation Scores
    cv_means = [results[name]['cv_mean_r2'] for name in model_names]
    cv_stds = [results[name]['cv_std_r2'] for name in model_names]
    
    axes[0,1].bar(model_names, cv_means, yerr=cv_stds, capsize=5, alpha=0.8, color=['skyblue', 'lightgreen', 'salmon'])
    axes[0,1].set_xlabel('Models')
    axes[0,1].set_ylabel('Cross-Validated R¬≤')
    axes[0,1].set_title('Cross-Validation Performance (5-fold)')
    axes[0,1].grid(True, alpha=0.3)
    
    # Actual vs Predicted Scatter (best model)
    best_model = max(results.keys(), key=lambda x: results[x]['test_r2'])
    predictions = results[best_model]['predictions']
    actual = results[best_model]['actual']
    
    axes[1,0].scatter(actual, predictions, alpha=0.6, s=20, color='blue')
    axes[1,0].plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'r--', lw=2)
    axes[1,0].set_xlabel('Actual Price (‚Ç¨)')
    axes[1,0].set_ylabel('Predicted Price (‚Ç¨)')
    axes[1,0].set_title(f'Best Model: {best_model} - Predictions vs Actual')
    axes[1,0].grid(True, alpha=0.3)
    
    # Residual Analysis for Best Model
    residuals = actual - predictions
    axes[1,1].scatter(predictions, residuals, alpha=0.6, s=20, color='green')
    axes[1,1].axhline(y=0, color='r', linestyle='--')
    axes[1,1].set_xlabel('Predicted Price (‚Ç¨)')
    axes[1,1].set_ylabel('Residuals (‚Ç¨)')
    axes[1,1].set_title(f'Best Model: {best_model} - Residual Analysis')
    axes[1,1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('yacht_ml_model_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return best_model

def feature_importance_analysis(results, feature_names):
    """Analyze and visualize feature importance for tree-based models"""
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # RandomForest Feature Importance
    if 'RandomForest' in results:
        rf_model = results['RandomForest']['model']
        rf_importance = pd.DataFrame({
            'feature': feature_names,
            'importance': rf_model.feature_importances_
        }).sort_values('importance', ascending=True).tail(15)
        
        axes[0].barh(rf_importance['feature'], rf_importance['importance'], color='lightblue')
        axes[0].set_title('RandomForest - Top 15 Features')
        axes[0].set_xlabel('Importance Score')
    
    # GradientBoosting Feature Importance
    if 'GradientBoosting' in results:
        gb_model = results['GradientBoosting']['model']
        gb_importance = pd.DataFrame({
            'feature': feature_names,
            'importance': gb_model.feature_importances_
        }).sort_values('importance', ascending=True).tail(15)
        
        axes[1].barh(gb_importance['feature'], gb_importance['importance'], color='lightgreen')
        axes[1].set_title('GradientBoosting - Top 15 Features')
        axes[1].set_xlabel('Importance Score')
    
    plt.tight_layout()
    plt.savefig('yacht_feature_importance.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return rf_importance, gb_importance

def generate_ml_report(results, best_model):
    """Generate comprehensive ML performance report"""
    report = f"""
# Advanced Yacht Price Prediction - ML Model Report

## üéØ Executive Summary
- **Dataset**: Enhanced yacht market data with 1000 samples
- **Best Model**: {best_model}
- **Best Performance**: R¬≤ = {results[best_model]['test_r2']:.4f}
- **Price Prediction Accuracy**: Average error ‚Ç¨{results[best_model]['test_mae']:,.0f}

## üìä Model Performance Comparison

| Model | CV R¬≤ (Mean ¬± Std) | Test R¬≤ | RMSE (‚Ç¨) | MAE (‚Ç¨) |
|-------|-------------------|---------|----------|---------|
"""
    
    for name in results.keys():
        report += f"| {name} | {results[name]['cv_mean_r2']:.4f} ¬± {results[name]['cv_std_r2']:.4f} | {results[name]['test_r2']:.4f} | {results[name]['test_rmse']:,.0f} | {results[name]['test_mae']:,.0f} |\n"
    
    report += f"""

## üèÜ Best Model Analysis: {best_model}

### Performance Metrics
- **Cross-Validation R¬≤**: {results[best_model]['cv_mean_r2']:.4f} ¬± {results[best_model]['cv_std_r2']:.4f}
- **Test Set R¬≤**: {results[best_model]['test_r2']:.4f}
- **Root Mean Square Error**: ‚Ç¨{results[best_model]['test_rmse']:,.0f}
- **Mean Absolute Error**: ‚Ç¨{results[best_model]['test_mae']:,.0f}

### Business Interpretation
- Model explains {results[best_model]['test_r2']*100:.1f}% of yacht price variation
- Average prediction error: ‚Ç¨{results[best_model]['test_mae']:,.0f}
- Typical prediction accuracy: ¬±{results[best_model]['test_mae']/np.mean(results[best_model]['actual'])*100:.1f}%

## üîß Model Selection Criteria

### Technical Performance
1. **R¬≤ Score**: Higher = Better explanatory power
2. **Cross-Validation**: Consistency across data splits
3. **Error Metrics**: Lower RMSE and MAE = Better accuracy
4. **Overfitting**: Similar train/test performance

### Business Considerations
1. **Interpretability**: Feature importance insights
2. **Prediction Accuracy**: Business-critical error margins
3. **Training Speed**: Real-world deployment constraints
4. **Robustness**: Performance on diverse yacht types

## üìà Next Steps: Hyperparameter Optimization

Ready for Phase 2: GridSearchCV optimization of best model
- **Expected Improvement**: 5-15% performance boost
- **Business Value**: Enhanced pricing accuracy
- **Deployment Ready**: API integration planned

---
*Generated by Advanced ML Training Pipeline*
"""
    
    with open('yacht_ml_performance_report.md', 'w') as f:
        f.write(report)
    
    print(f"‚úÖ ML Performance Report generated: 'yacht_ml_performance_report.md'")
    
    return report

def main():
    """Main ML training pipeline"""
    print("üöÄ Starting Advanced Yacht Price ML Training...\n")
    
    # Load and prepare data
    X, y, feature_names = load_data()
    print(f"‚úÖ Data loaded: {X.shape[0]} samples, {X.shape[1]} features\n")
    
    # Train models
    results = train_models(X, y)
    
    # Visualize results
    print("\nüìà Creating model comparison visualizations...")
    best_model = visualize_model_comparison(results)
    
    # Feature importance
    print("üîç Analyzing feature importance...")
    rf_imp, gb_imp = feature_importance_analysis(results, feature_names)
    
    # Generate report
    print("üìã Generating comprehensive ML performance report...")
    report = generate_ml_report(results, best_model)
    
    print(f"\nüéâ ML Training Complete!")
    print(f"üèÜ Best Model: {best_model}")
    print(f"üìä Performance: R¬≤ = {results[best_model]['test_r2']:.4f}")
    print(f"üí∞ Average Error: ‚Ç¨{results[best_model]['test_mae']:,.0f}")
    print(f"\nüìÅ Generated Files:")
    print(f"  - yacht_ml_model_comparison.png")
    print(f"  - yacht_feature_importance.png") 
    print(f"  - yacht_ml_performance_report.md")
    
    return results, best_model, feature_names

if __name__ == "__main__":
    results, best_model, feature_names = main()